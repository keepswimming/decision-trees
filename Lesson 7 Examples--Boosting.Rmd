---
title: "Lesson 7 Examples--Boosting"
author: "Abra Brisbin"
date: "7/4/2021"
output: word_document
always_allow_html: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(gbm)
library(dplyr)
library(ggformula)
```

# Using boosted trees to predict whether an object is rock or metal based on its sonar readings

```{r}
sonar = read.csv("Sonar_rock_metal.csv", header = FALSE)
```




Re-define V61 as a numerical variable:
```{r}
sonar <- sonar %>%
  mutate(object = case_when(V61 == "M" ~ 1,
                            V61 == "R" ~ 0,
                            TRUE ~ NA_real_)) %>%
  select(-V61)
  # Alternatively, could use object ~ .-V61 in gbm.
```






```{r}
set.seed(729)
boost = gbm(object ~ ., data = sonar, 
            distribution = "bernoulli", n.trees = 1000,
            shrinkage = .001, interaction.depth = 3)
  # Use "gaussian" for a regression problem

#predict(boost, newdata = sonar, n.trees = 1000, type = "response")

boost
```




## Investigate the model
```{r}
boost_summary = summary(boost)
boost_summary
```










```{r}
boost_summary %>%
  mutate(var = reorder(var, -rel.inf)) %>%
  head(n = 10) %>%
  gf_col(rel.inf ~ var) %>%
  gf_labs(title = "Relative Influence of top 10 variables in `sonar`")
```







### Marginal effects
```{r}
plot(boost, i = "V11") # Higher values of V11 
                       # are predictive of metal
plot(boost, i = "V36")
```







## Use 10-fold cross-validation to estimate the error rate.
```{r}
n = 208
k = 10 # Using 10-fold CV
groups = rep(1:k, length = n)
groups
```


```{r}
set.seed(729)
cvgroups = sample(groups, n)
boost_predict = numeric(length = n)
data_used = sonar

for(ii in 1:k){
  groupi = (cvgroups == ii)
  # Perform boosting on everything not in groupi
  # Then predict values for groupi
  boost = gbm(object ~ ., data = data_used[!groupi, ], 
            distribution = "bernoulli", n.trees = 1000,
            shrinkage = .001, interaction.depth = 3)
  

  boost_predict[groupi] = predict(boost, newdata = data_used[groupi, ], n.trees = 1000, type = "response")
}
```

```{r}
boost_predict[1:5]
```






Confusion matrix:
```{r}
conf_mat = table(boost_predict > .5, sonar$object)
conf_mat
```










```{r}
sum(diag(conf_mat)) / n # Accuracy
1- sum(diag(conf_mat))/n # Classification error rate
```











## Using cross-validation to test different interaction depths
This is similar to the code used to create the "Interaction Depth" slide, but for the `sonar` data set and a classification problem (so we're graphing the classification error instead of the MSE).
```{r, CV}
n = 208
k = 10 # Using 10-fold CV
groups = rep(1:k, length = n)

set.seed(729)
cvgroups = sample(groups, n)
boost_predict = numeric(length = n)
data_used = sonar

depths = 1:20
error = numeric(length = length(depths))

for(jj in 1:length(depths)){
  for(ii in 1:k){
    groupi = (cvgroups == ii)
    # Perform boosting on everything not in groupi
    boost = gbm(object ~ ., 
                data = data_used[!groupi, ],
                distribution = "bernoulli", 
                n.trees = 1000, 
                shrinkage = .001, 
                interaction.depth = depths[jj])
    
    # Then predict values for groupi
    boost_predict[groupi] = predict(boost, 
                  newdata = data_used[groupi, ],
                  n.trees = 1000, type = "response")
  } # end iteration over folds
  
  # Compute error for this interaction depth
  conf_mat = table(boost_predict > .5, data_used$object)
  error[jj] = 1 - sum(diag(conf_mat))/n
} # end iteration over interaction depths
```

```{r, graph_error}
gf_point(error ~ depths)
```







# Boosting in caret
When doing boosting in caret, I like to use `XGBoost`, as it tends to be faster than `gbm`.  This is helpful when I want to use caret to test many combinations of tuning parameters.
```{r, message = FALSE}
library(caret)
library(xgboost)
library(ISLR) # for College data, used in hwk 4
```

## Predicting Enrollment at colleges

```{r}
head(College)
```

We'll use 100 trees (`XGBoost` typically works fine with fewer trees than `gbm`) with depths of 1, 2, or 3.  `eta` is the learning rate, similar to $\lambda$ in `gbm`.  0.3 is usually a good starting point for it.  `subsample` is the bag fraction.
```{r, xgboost}

set.seed(123)
ctrl = trainControl(method = "cv", number = 5)
fit_coll = train(Enroll ~ ., 
             data = College,
             method = "xgbTree",
             tuneGrid = expand.grid(nrounds = 100, max_depth = 1:3, 
                                    eta = 0.3, gamma = 0, 
                                    colsample_bytree = .8, 
                                    min_child_weight = 1, subsample = 1),
             verbosity = 0, # suppresses a warning about 
                            # deprecated version of the 
                            # objective function
                            # (not necessary for classification)
             trControl = ctrl)

fit_coll

```
View the trees.  (Note that this will only show up in the knitted document if you knit to HTML.)
```{r, show_tree}
#xgb.plot.tree(model = fit_coll$finalModel) # would show all trees
xgb.plot.tree(model = fit_coll$finalModel, trees = 0) # show the 0th tree
```

### Variable importance
```{r}

xgb.importance(model = fit_coll$finalModel)

```

```{r}

varImp(fit_coll)

```

```{r}

import_matrix = xgb.importance(model = fit_coll$finalModel)
import_matrix
xgb.plot.importance(import_matrix)

```

### Display the depth of the trees
```{r}
xgb.plot.deepness(model = fit_coll$finalModel)
```